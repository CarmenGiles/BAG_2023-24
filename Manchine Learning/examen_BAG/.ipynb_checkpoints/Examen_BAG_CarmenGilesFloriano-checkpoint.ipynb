{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Proyecto de análisis de sentimientos con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Carmen Giles Floriano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se realizará un modelo de aprendizaje automático capaz de analizar tweets y predecir el sentimiento del usuario dentro de las siguientes categorías: \"Muy feliz\", \"Contento\", \"Neutro\", \"Molesto\" y \"Hater\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ejercicio 1. RECOPILACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "import numpy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se lee el fichero `fifa_tweets_emotion.csv` mediante el paquete pandas. Los datos fueron obtenidos de Kaggle y son un conjunto de tweets relacionados con la WorldCup del FIFA 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22525, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>number_likes</th>\n",
       "      <th>source</th>\n",
       "      <th>tweet</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Date Created</td>\n",
       "      <td>Number of Likes</td>\n",
       "      <td>Source of Tweet</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-11-20 23:59:21+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-20 23:59:01+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-11-20 23:58:41+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-11-20 23:58:33+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number                       date     number_likes              source  \\\n",
       "0     NaN               Date Created  Number of Likes     Source of Tweet   \n",
       "1     0.0  2022-11-20 23:59:21+00:00                4     Twitter Web App   \n",
       "2     1.0  2022-11-20 23:59:01+00:00                3  Twitter for iPhone   \n",
       "3     2.0  2022-11-20 23:58:41+00:00                1  Twitter for iPhone   \n",
       "4     3.0  2022-11-20 23:58:33+00:00                1     Twitter Web App   \n",
       "\n",
       "                                               tweet    feeling  \n",
       "0                                              Tweet  Sentiment  \n",
       "1  What are we drinking today @TucanTribe \\n@MadB...    neutral  \n",
       "2  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   positive  \n",
       "3  Worth reading while watching #WorldCup2022 htt...   positive  \n",
       "4  Golden Maknae shinning bright\\n\\nhttps://t.co/...   positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con la función \"read_csv\" del paquete \"pandas\", leemos el data set indicando los nombres de las columnas.\n",
    "tweet_fifa_prev = pandas.read_csv('fifa_tweets_emotion.csv', header=None,\n",
    "                       names=['number', 'date', 'number_likes','source', 'tweet','feeling'])\n",
    "print(tweet_fifa_prev.shape)\n",
    "tweet_fifa_prev.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El data set presenta 22525 tweets distintos, cada uno representado en una fila. Además, del texto del propio tweet existen otras columnas con más información, como la fecha de publicación, el número de likes de la publicación, el lugar desde dónde se publicó y el sentimiento asociado. \n",
    "\n",
    "En este proyecto, sólo nos interesa el texto correspondiente al tweet. Los demás datos son eliminados, incluso la columna correspondiente a los sentimientos, ya que estos serán asignados siguiendo el criterio de la herramienta TextBlob. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "1  What are we drinking today @TucanTribe \\n@MadB...\n",
       "2  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...\n",
       "3  Worth reading while watching #WorldCup2022 htt...\n",
       "4  Golden Maknae shinning bright\\n\\nhttps://t.co/...\n",
       "5  If the BBC cares so much about human rights, h..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con la función \"drop\" se eliminan elementos del data set.\n",
    "## En la lista \"delete_col\" añadimos los nombres de las columnas que deseamos eliminar, la cuál será un parámetro de la función\n",
    "## \"drop\". Además, a esta función se le da el parámetro axis=1, lo que indica que los elementos a eliminar son columnas.\n",
    "\n",
    "delete_col=[\"number\", \"date\", \"number_likes\", \"source\", \"feeling\"]\n",
    "tweet_fifa_col= tweet_fifa_prev.drop(delete_col, axis=1)\n",
    "\n",
    "## Por otro lado, eliminamos la primera fila que contiene los anteriores nombres de las columnas del data set. Se elimina \n",
    "## indicando 0 (primer elemento) y el parámetro axis=0 (fila). \n",
    "\n",
    "tweet_fifa=tweet_fifa_col.drop(0, axis=0)\n",
    "tweet_fifa.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2. LIMPIEZA DEL TEXTO, ELIMINAR LAS PALABRAS QUE NO APORTAN INFORMACIÓN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, procesamos el texto del tweet de forma que sea más fácil de codificar y analizar en pasos posteriores. Para ello se crea la función \"limpiar_texto\", que elimina elementos como menciones, hashtags, URLs y emoticonos; convierte el texto en minúscula; elimina las palabras poco informativas; y, además, realizará una lematización, es decir, transforma las palabras a su forma base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recibe como entrada un conjunto de datos (data set).\n",
    "def limpiar_texto(data_set):\n",
    "    \n",
    "    ## En primer lugar, se cargan una serie de datos necesarios para el posterior procesamiento:\n",
    "    \n",
    "    # Se cargan los datos que hacen referencia a distintos emoticonos, utilizando \"re\", un módulo de la biblioteca de Pyhton. \n",
    "    # En concreto, con la funcion \"compile\" se almacena dicha información indicando el código Unicode del bloque de emoticonos\n",
    "    # correspondiente. El parámetro \"flags=re.UNICODE\" indica que la información añadida se lea según las reglas de Unicode, \n",
    "    # para que sean identificados como emoticonos.\n",
    "    patron_emoticonos = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # Emoticonos generales\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # Símbolos y pictogramas\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # Transporte y mapas\n",
    "                            u\"\\U0001F780-\\U0001F7FF\"  # Formas geométricas extendidas\n",
    "                            u\"\\U0001F800-\\U0001F8FF\"  # Signos de puntuación\n",
    "                            u\"\\U00002702-\\U000027B0\"  # Símbolos diversos, como tijeras o meteorológicos\n",
    "                            u\"\\U000024C2-\\U0001F251\"  # Transporte, pictogramas y otros símbolos\n",
    "                            u\"\\U0001F900-\\U0001F9FF\"  # Emoticonos de personas y cuerpos\n",
    "                            u\"\\U0001FA00-\\U0001FA6F\"  # Símbolos de objetos\n",
    "                            u\"\\U0001FA70-\\U0001FAFF\"  # Símbolos de alimentos\n",
    "                            u\"\\U0001F1E6-\\U0001F1FF\"  # Banderas \n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    # Se carga la información de la función \"TweetTokenize\" del paquete \"nltk\". \n",
    "    tokenizer=TweetTokenizer()\n",
    "    \n",
    "    # Se carga el conjunto de palabras \"stopwords\" del paquete nltk, estas son palabras muy frecuentes en el lenguaje pero poco\n",
    "    # informativas. \n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords_english = stopwords.words(\"english\") # Se selecciona el idioma inglés.\n",
    "    \n",
    "    # Se carga la información necesaria para la lematización mediante la función \"SnowballStemmer\" del paquete \"nltk\".\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    ## Se inicia un diccionario que alamacenará los tweets procesados.\n",
    "    diccionario = {\"tweet\": []}\n",
    "    \n",
    "    ## Para cada fila del data set: \n",
    "    for index, fila in data_set.iterrows():\n",
    "        \n",
    "        ## En la variable fila se tiene la información de la fila completa, al seleccionar \"tweet\", nos quedamos sólo con la \n",
    "        ## información de esa columna, es decir, el texto del tweet correspondiente.\n",
    "        tweet=fila[\"tweet\"]\n",
    "        \n",
    "        ## Con la función \"sub\" del módulo \"re\" se eliminan una serie de elementos, indicados en el primer parámetro de la \n",
    "        ## función (r'elemento'). \n",
    "        \n",
    "        # Se eliminan los hashtags (#)\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        \n",
    "        # Se eliminan las menciones, las cuáles tienen la siguiente estructura: @usuario. Al añadir \\S+, se indica que también \n",
    "        # se elimina cualquier texto que esté inmediatamente después del elemento determinado, sin incluir espacios.\n",
    "        tweet = re.sub(r'@\\S+', '', tweet)\n",
    "        \n",
    "        # Se eliminan los URLs, que por lo general comienzan con http.\n",
    "        tweet = re.sub(r'http\\S+', '', tweet) \n",
    "        \n",
    "        # Se eliminan los emoticonos usando el conjunto de emoticonos anteriormente guardados.\n",
    "        tweet = patron_emoticonos.sub(r'',tweet)\n",
    "        \n",
    "        ## Se transforma todo el texto a minúscula.\n",
    "        tweet=tweet.lower()\n",
    "        \n",
    "        ## Posteriormente, se procesará el texto palabra por palabra. Para ello, en primer lugar, se debe tokenizar el texto, \n",
    "        ## es decir, separar las palabras a elementos de una lista. Se realiza mediante la función \"TweetTokenizer\" cargada \n",
    "        ## anteriormente.\n",
    "        tweet= tokenizer.tokenize(tweet)\n",
    "        ## Se inicia una lista que contendrá las palabras procesadas del tweet.\n",
    "        list_tweet=[]\n",
    "\n",
    "        ## Para cada palabra:\n",
    "        for palabra in tweet:\n",
    "            ## Si la palabra NO está en el conjunto \"stopwords_english\", palabras con poco valor informativo:\n",
    "            if palabra not in stopwords_english:\n",
    "                ## Se transforma a su forma base con la función \"SnowballStemmer\".\n",
    "                palabra_proc = stemmer.stem(palabra)\n",
    "                ## La palabra ya procesada se añade a la lista anteriormente creada.\n",
    "                list_tweet.append(palabra_proc)\n",
    "        \n",
    "        ## Una vez procesadas las palabras del tweet, se vuelven a unir en la cadena que se inicia a continuación.\n",
    "        cadena_tweet=\"\"\n",
    "        \n",
    "        ## Para cada palabra:\n",
    "        for palabra in list_tweet:\n",
    "            ## Se añade cada palabra a la cadena anteriormente creada, separadas por espacios.\n",
    "            cadena_tweet+=\" \"\n",
    "            cadena_tweet+=palabra\n",
    "        \n",
    "        ## El tweet procesado se añade al diccionario    \n",
    "        diccionario[\"tweet\"].append(cadena_tweet)\n",
    "    \n",
    "    ## Se convierte el diccionario en un data set, mediante el paquete \"pandas\", y se devuelve.\n",
    "    data_set_proc=pandas.DataFrame(diccionario)\n",
    "    return data_set_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0                          drink today worldcup 2022\n",
       "1   amaz worldcup 2022 launch video . show much f...\n",
       "2                     worth read watch worldcup 2022\n",
       "3   golden makna shin bright jeonjungkook jungkoo...\n",
       "4   bbc care much human right , homosexu right , ..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fifa_limp=limpiar_texto(tweet_fifa)\n",
    "tweet_fifa_limp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se obtienen un data set con tweets libres de emoticonos, de palabras pocas informativas y de otros elementos que no interesan a la hora de analizar el sentimiento de un tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Ejercicio 3. ETIQUETADO DE DATOS CON HERRAMIENTAS YA EXISTENTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se creará una función denominada \"clasificador\" con la cuál se asignará un sentimiento a cada tweet. Esto se realizará con el modelo \"TextBlob\" ya existente. Este asigna una puntuación (polaridad) a un texto dado, desde -1 a 1. Una puntuación cercana a -1 hace referencia a un texto con un sentimiento muy negativo y, cercana a 1, muy positivo. De esta forma, se asignan los sentimientos de la siguiente forma:\n",
    "\n",
    "Muy feliz: polaridad=[-1,-0.6)\n",
    "\n",
    "Contento: polaridad=[-0.6, -0.2)\n",
    "\n",
    "Neutro: polaridad=[-0.2, 0.2)\n",
    "\n",
    "Molesto: polaridad=[0.2, 0.6)\n",
    "\n",
    "Hater: polaridad=[0.6, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Como entrada la función recibe un data set.\n",
    "def clasificador(data_set):\n",
    "    \n",
    "    ## Se inicia un diccionario vacio, para guardar, por un lado, el tweet y, por otro lado, el sentimiento asociado.\n",
    "    diccionario={\"tweet\":[], \"sentimiento\":[]}\n",
    "    \n",
    "    ## Para cada fila del data set:\n",
    "    for index, fila in data_set.iterrows():\n",
    "        \n",
    "        ## Se extrae el texto del tweet correspondiente.\n",
    "        tweet=fila[\"tweet\"]\n",
    "        \n",
    "        ## Se analiza con el paquete TextBlob.\n",
    "        texto=TextBlob(tweet)\n",
    "        \n",
    "        ## Se extrae el valor de polaridad que, como se explicó anteriormente, está asociado a un sentimiento. Siguiendo el \n",
    "        ## criterio anteriormente marcado, dependiendo del valor de polaridad calculado se asigna un sentimiento.\n",
    "        sentimiento_pol=texto.sentiment.polarity\n",
    "        if sentimiento_pol < (-0.6):\n",
    "            sentimiento=\"Hater\"\n",
    "        elif (-0.6) <= sentimiento_pol < (-0.2):\n",
    "            sentimiento=\"Molesto\"\n",
    "        elif (-0.2) <= sentimiento_pol < 0.2:\n",
    "            sentimiento=\"Neutro\"\n",
    "        elif 0.2 <= sentimiento_pol < 0.6:\n",
    "            sentimiento=\"Contento\"\n",
    "        else:\n",
    "            sentimiento=\"Muy feliz\"\n",
    "        \n",
    "        ## El tweet se almacena en el diccionario.\n",
    "        diccionario[\"tweet\"].append(tweet)\n",
    "        \n",
    "        ## El sentimiento, guardado en una cadena de carácteres, se almacena en el diccionario, generando una lista de cadenas.\n",
    "        diccionario[\"sentimiento\"].append(sentimiento)\n",
    "    \n",
    "    ## El diccionario se convierte en un data set y se devuelve.\n",
    "    data_set_sent=pandas.DataFrame(diccionario)\n",
    "    return data_set_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentimiento\n",
       "0                          drink today worldcup 2022      Neutro\n",
       "1   amaz worldcup 2022 launch video . show much f...      Neutro\n",
       "2                     worth read watch worldcup 2022    Contento\n",
       "3   golden makna shin bright jeonjungkook jungkoo...    Contento\n",
       "4   bbc care much human right , homosexu right , ...      Neutro"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fifa_sent=clasificador(tweet_fifa_limp)\n",
    "tweet_fifa_sent.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, se obtiene un data set con dos columnas, una con los tweets limpios y otra con el sentimiento asociado a dicho tweet. Este conjunto de datos será codificado y usado para entrenar un modelo propio, que permita asignar dichos sentimientos a nuevos tweets de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4. CODIFICACIÓN DE LOS ATRIBUTOS Y OBJETIVOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El siguiente paso para crear un modelo, es codificar los tweets a lenguaje numérico. Existen múltiples codificadores con distintas características. Por ejemplo, se tiene el codificador **CountVectorizer**, el cuál se basa en realizar recuentos de las distintas palabras, sin tener en cuenta la semántica o la posición de estas. Por esa razón, se buscó otras opciones más apropiadas para analizar tweets, dónde el contexto de las palabras es importante. Se estudiará el codificador **Word2Vec**, el cuál sí que tiene en cuenta la relación entre las palabras, esto es útil, por ejemplo, para poder procesar expresiones o frases hechas. Este modelo asignará a cada palabra un vector con unas dimensiones determinadas. El modelo se basa en que las palabras que se parecen estarán representadas por vectores cercanos en el espacio. A continuación, vemos como se realiza una codificación de este tipo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### CODIFICADOR WORD2VEC\n",
    "\n",
    "## Se obtiene la lista de tweets procesados.\n",
    "tweets=tweet_fifa_sent[\"tweet\"]\n",
    "\n",
    "## Este codificador necesita que el texto este tokenizado, es decir, que cada palabra sea un elemento de una lista. Esto se \n",
    "## aplica para cada tweet, mediante la función \"TweetTokenizer\" del paquete \"nltk\" y se almacena en una lista.\n",
    "tokens_list=[]\n",
    "tokenizer=TweetTokenizer()\n",
    "for tweet in tweets:\n",
    "    tokens= tokenizer.tokenize(tweet)\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "\n",
    "## Se entrena el modelo de codificación Word2Vec, utilizando la lista de tokens previamente generada. El parámetro \n",
    "## \"vector_size=100\" indica que el vector que representa cada palabra tiene 100 dimensiones; por otro lado, windows=7 \n",
    "## indica que considera 7 palabras a cada lado de la analizada para entender el contexto; y min_count=5 indica que dicha \n",
    "## palabra aparecer mínimo una vez para aparecer en el modelo. \n",
    "model = Word2Vec(tokens_list, vector_size=100, window=7, min_count=5)\n",
    "\n",
    "## Se inicia un diccionario que contendrá la codificación para cada tweet.\n",
    "atributo={\"tweet_vector\":[]}\n",
    "\n",
    "## Para cada tweet tokenizado:\n",
    "for tweet in tokens_list:\n",
    "    lista=[]\n",
    "    ## Para cada palabra de dicho tweet:\n",
    "    for palabra in tweet:\n",
    "        ## Si la palabra está presente en el modelo previamente creado:\n",
    "        if palabra in model.wv:\n",
    "            ## Se realiza la codificación, o vector, de dicha palabra y se añade a la lista anteriormente creada.\n",
    "            codificacion = model.wv[palabra]\n",
    "            lista.append(codificacion)\n",
    "    ## Si no está la lista vacia:\n",
    "    if lista:\n",
    "        ## Se calcula el promedio de todos los vectores para tener una representación única del tweet.\n",
    "        media_vector=sum(lista)/len(lista)\n",
    "        ## Dicha media se añade al diccionario \"atributos\".\n",
    "        atributo[\"tweet_vector\"].append(media_vector)\n",
    "    ## Si está vacia:\n",
    "    else:\n",
    "        ## Se añade un elemento vacio a la lista.\n",
    "        atributo[\"tweet_vector\"].append([])\n",
    "\n",
    "##         \n",
    "atributo=pandas.DataFrame(atributo)\n",
    "\n",
    "objetivo=tweet_fifa_sent[\"sentimiento\"]\n",
    "\n",
    "(atributos_entrenamiento, atributos_prueba,\n",
    " objetivo_entrenamiento, objetivo_prueba) = train_test_split(\n",
    "       atributo, objetivo,\n",
    "       random_state=12345,\n",
    "       test_size=.2,\n",
    "       stratify=objetivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos_1=tweet_fifa_sent[\"tweet\"]\n",
    "vectorizer = CountVectorizer()\n",
    "atributos_1 = vectorizer.fit_transform(atributos_1)\n",
    "\n",
    "#Extrae el objetivo (ya es numérico)\n",
    "objetivo_1= tweet_fifa_sent[\"sentimiento\"]\n",
    "#Separa conjunto de entrenamiento y de prueba\n",
    "(atributos_entrenamiento_1, atributos_prueba_1,\n",
    " objetivo_entrenamiento_1, objetivo_prueba_1) = train_test_split(\n",
    "        atributos_1, objetivo_1,\n",
    "        random_state=12345,\n",
    "        test_size=.2,\n",
    "        stratify=objetivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo Random Forest es 71.8312985571587\n"
     ]
    }
   ],
   "source": [
    "#### RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Entrenar el modelo Random Forest\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(list(atributos_entrenamiento[\"tweet_vector\"]), objetivo_entrenamiento)\n",
    "\n",
    "# Realizar predicciones con el conjunto de prueba\n",
    "predicciones_rf = rf_classifier.predict(list(atributos_prueba[\"tweet_vector\"]))\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "precision_rf = numpy.mean(predicciones_rf == objetivo_prueba)\n",
    "print(\"La precisión del modelo Random Forest es\", precision_rf * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo Naive Bayes desarrollado es 71.03218645948945\n"
     ]
    }
   ],
   "source": [
    "#### NAIVE BAYES\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Entrena el modelo de Naive Bayes usando la instancia MultinomialNB que es recomendada \n",
    "#para este tipo de tareas\n",
    "emotion_detector = MultinomialNB(alpha=1.0)  # alpha es el parámetro de suavizado\n",
    "emotion_detector.fit(atributos_entrenamiento_1, objetivo_entrenamiento_1)\n",
    "\n",
    "#Realiza las predicciones con el conjunto de prueba\n",
    "predicciones = emotion_detector.predict(atributos_prueba_1)\n",
    "#Calcular la precisión del modelo\n",
    "precision = emotion_detector.score(atributos_prueba_1, objetivo_prueba_1)\n",
    "print(\"La precisión del modelo Naive Bayes desarrollado es\", precision*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "name": "Solución_01_nueva.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
