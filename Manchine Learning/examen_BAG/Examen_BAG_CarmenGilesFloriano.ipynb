{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Proyecto de análisis de sentimientos con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Carmen Giles Floriano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se realizará un modelo de aprendizaje automático capaz de analizar tweets y predecir el sentimiento del usuario dentro de las siguientes categorías: \"Muy feliz\", \"Contento\", \"Neutro\", \"Molesto\" y \"Hater\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ejercicio 1. RECOPILACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "#import numpy\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se lee el fichero `fifa_tweets_emotion.csv` mediante el paquete pandas. Los datos fueron obtenidos de Kaggle y son un conjunto de tweets relacionados con la WorldCup del FIFA 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22525, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>number_likes</th>\n",
       "      <th>source</th>\n",
       "      <th>tweet</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Date Created</td>\n",
       "      <td>Number of Likes</td>\n",
       "      <td>Source of Tweet</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-11-20 23:59:21+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-20 23:59:01+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-11-20 23:58:41+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-11-20 23:58:33+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number                       date     number_likes              source  \\\n",
       "0     NaN               Date Created  Number of Likes     Source of Tweet   \n",
       "1     0.0  2022-11-20 23:59:21+00:00                4     Twitter Web App   \n",
       "2     1.0  2022-11-20 23:59:01+00:00                3  Twitter for iPhone   \n",
       "3     2.0  2022-11-20 23:58:41+00:00                1  Twitter for iPhone   \n",
       "4     3.0  2022-11-20 23:58:33+00:00                1     Twitter Web App   \n",
       "\n",
       "                                               tweet    feeling  \n",
       "0                                              Tweet  Sentiment  \n",
       "1  What are we drinking today @TucanTribe \\n@MadB...    neutral  \n",
       "2  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   positive  \n",
       "3  Worth reading while watching #WorldCup2022 htt...   positive  \n",
       "4  Golden Maknae shinning bright\\n\\nhttps://t.co/...   positive  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con la función \"read_csv\" del paquete \"pandas\", leemos el data set indicando los nombres de las columnas.\n",
    "tweet_fifa_prev = pandas.read_csv('fifa_tweets_emotion.csv', header=None,\n",
    "                       names=['number', 'date', 'number_likes','source', 'tweet','feeling'])\n",
    "print(tweet_fifa_prev.shape)\n",
    "tweet_fifa_prev.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El data set presenta 22525 tweets distintos, cada uno representado en una fila. Además, del texto del propio tweet existen otras columnas con más información, como la fecha de publicación, el número de likes de la publicación, el lugar desde dónde se publicó y el sentimiento asociado. \n",
    "\n",
    "En este proyecto, sólo nos interesa el texto correspondiente al tweet. Los demás datos son eliminados, incluso la columna correspondiente a los sentimientos, ya que estos serán asignados siguiendo el criterio de la herramienta TextBlob. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "1  What are we drinking today @TucanTribe \\n@MadB...\n",
       "2  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...\n",
       "3  Worth reading while watching #WorldCup2022 htt...\n",
       "4  Golden Maknae shinning bright\\n\\nhttps://t.co/...\n",
       "5  If the BBC cares so much about human rights, h..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con la función \"drop\" se eliminan elementos del data set.\n",
    "## En la lista \"delete_col\" añadimos los nombres de las columnas que deseamos eliminar, la cuál será un parámetro de la función\n",
    "## \"drop\". Además, a esta función se le da el parámetro axis=1, lo que indica que los elementos a eliminar son columnas.\n",
    "\n",
    "delete_col=[\"number\", \"date\", \"number_likes\", \"source\", \"feeling\"]\n",
    "tweet_fifa_col= tweet_fifa_prev.drop(delete_col, axis=1)\n",
    "\n",
    "## Por otro lado, eliminamos la primera fila que contiene los anteriores nombres de las columnas del data set. Se elimina \n",
    "## indicando 0 (primer elemento) y el parámetro axis=0 (fila). \n",
    "\n",
    "tweet_fifa=tweet_fifa_col.drop(0, axis=0)\n",
    "tweet_fifa.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2. LIMPIEZA DEL TEXTO, ELIMINAR LAS PALABRAS QUE NO APORTAN INFORMACIÓN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, procesamos el texto del tweet de forma que sea más fácil de codificar y analizar en pasos posteriores. Para ello se crea la función \"limpiar_texto\", que elimina elementos como menciones, hashtags, URLs y emoticonos; convierte el texto en minúscula; elimina las palabras poco informativas; y, además, realizará una lematización, es decir, se transformarán las palabras a su forma base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recibe como entrada un data set\n",
    "def limpiar_texto(data_set):\n",
    "    ## En primer lugar, se cargan una serie de datos necesarios para el posterior procesamiento:\n",
    "    # Se cargan los datos de emoticonos con el paquete \"re\" e indicando el código, unicode, del bloque de emoticonos \n",
    "    # correspondiente. El parámetro \"flags=re.UNICODE\" indica que la información añadida se lea según las reglas de unicode,es \n",
    "    # decir, para que sean identificados como emoticonos.\n",
    "    patron_emoticonos = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # Emoticonos generales\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # Símbolos y pictogramas\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # Transporte y mapas\n",
    "                            u\"\\U0001F780-\\U0001F1FF\"  # Formas geométricas extendidas\n",
    "                            u\"\\U0001F900-\\U0001F9FF\"  # Emoticonos de personas y cuerpos\n",
    "                            u\"\\U0001FA00-\\U0001FA6F\"  # Símbolos de objetos\n",
    "                            u\"\\U0001FA70-\\U0001FAFF\"  # Símbolos de alimentos\n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    tokenizer=TweetTokenizer()\n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords_english = stopwords.words(\"english\")\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    diccionario = {\"tweet\": []}\n",
    "    \n",
    "    ## Para cada fila del data set \n",
    "    for index, fila in data_set.iterrows():\n",
    "        \n",
    "        ## En la variable fila se tiene la información de la fila completa, al seleccionar \"tweet\", nos quedamos sólo con la \n",
    "        ## información de esa columna, es decir el texto del tweet correspondiente.\n",
    "        tweet=fila[\"tweet\"]\n",
    "        \n",
    "        ## Con la función \"sub\" del paquete \"re\", se eliminan una serie de elementos indicados en el primer parámetro de la \n",
    "        ## función (r'elemento'). \n",
    "        # Se eliminan los hashtags (#)\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        # Se eliminan las menciones, las cuáles tienen la siguiente estructura: @usuario. Al añadie \\S+, se indica que también \n",
    "        #se elimina cualquier texto que esté inmediatamente después del elemento determinado, sin incluir espacios.\n",
    "        tweet = re.sub(r'@\\S+', '', tweet)\n",
    "        # Se eliminan los URLs, que por lo general comienzan con http.\n",
    "        tweet = re.sub(r'http\\S+', '', tweet) \n",
    "        # Se eliminan los emoticonos usando el conjunto de emoticonos guardados, anteriormente, en \"patrón_emoticonos\".\n",
    "        tweet = patron_emoticonos.sub(r'',tweet)\n",
    "        \n",
    "        # Poner en minuscula\n",
    "        tweet=tweet.lower()\n",
    "        \n",
    "        tweet= tokenizer.tokenize(tweet)\n",
    "        list_tweet=[]\n",
    "        for palabra in tweet:\n",
    "            if palabra not in stopwords_english:\n",
    "                palabra_proc = stemmer.stem(palabra)\n",
    "                list_tweet.append(palabra_proc)\n",
    "        \n",
    "        cadena_tweet=\"\"\n",
    "        for palabra in list_tweet:\n",
    "            cadena_tweet+=\" \"\n",
    "            cadena_tweet+=palabra\n",
    "            \n",
    "        diccionario[\"tweet\"].append(cadena_tweet)\n",
    "    data_set_proc=pandas.DataFrame(diccionario)\n",
    "    return data_set_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0                          drink today worldcup 2022\n",
       "1   amaz worldcup 2022 launch video . show much f...\n",
       "2                     worth read watch worldcup 2022\n",
       "3   golden makna shin bright jeonjungkook jungkoo...\n",
       "4   bbc care much human right , homosexu right , ..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fifa_limp=limpiar_texto(tweet_fifa)\n",
    "tweet_fifa_limp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Ejercicio 3. ETIQUETADO DE DATOS CON HERRAMIENTAS YA EXISTENTES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasificador(data_set):\n",
    "    diccionario={\"tweet\":[], \"sentimiento\":[]}\n",
    "    for index, fila in data_set.iterrows():\n",
    "        tweet=fila[\"tweet\"]\n",
    "        texto=TextBlob(tweet)\n",
    "        sentimiento_pol=texto.sentiment.polarity\n",
    "        if sentimiento_pol < (-0.6):\n",
    "            sentimiento=\"Hater\"\n",
    "        elif (-0.6) <= sentimiento_pol < (-0.2):\n",
    "            sentimiento=\"Molesto\"\n",
    "        elif (-0.2) <= sentimiento_pol < 0.2:\n",
    "            sentimiento=\"Neutro\"\n",
    "        elif 0.2 <= sentimiento_pol < 0.6:\n",
    "            sentimiento=\"Contento\"\n",
    "        else:\n",
    "            sentimiento=\"Muy feliz\"\n",
    "        diccionario[\"tweet\"].append(tweet)\n",
    "        diccionario[\"sentimiento\"].append(sentimiento)\n",
    "    data_set_sent=pandas.DataFrame(diccionario)\n",
    "    return data_set_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentimiento\n",
       "0                          drink today worldcup 2022      Neutro\n",
       "1   amaz worldcup 2022 launch video . show much f...      Neutro\n",
       "2                     worth read watch worldcup 2022    Contento\n",
       "3   golden makna shin bright jeonjungkook jungkoo...    Contento\n",
       "4   bbc care much human right , homosexu right , ...      Neutro"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fifa_sent=clasificador(tweet_fifa_limp)\n",
    "tweet_fifa_sent.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4. CODIFICACIÓN DE LOS ATRIBUTOS Y OBJETIVOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo Random Forest es 71.12097669256381\n"
     ]
    }
   ],
   "source": [
    "tweets=tweet_fifa_sent[\"tweet\"]\n",
    "tokens_list=[]\n",
    "for tweet in tweets:\n",
    "    tokens = word_tokenize(tweet)\n",
    "    tokens_list.append(tokens)\n",
    "    \n",
    "# Entrenamiento de Word2Vec\n",
    "model = Word2Vec(tokens_list, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Obtención del embedding de una palabra\n",
    "\n",
    "atributo={\"tweet_vector\":[]}\n",
    "for tweet in tokens_list:\n",
    "    vector=[]\n",
    "    for palabra in tweet:\n",
    "        if palabra in model.wv:\n",
    "            embedding = model.wv[palabra]\n",
    "            vector.append(embedding)\n",
    "    if vector:\n",
    "        media_vector=sum(vector)/len(vector)\n",
    "        atributo[\"tweet_vector\"].append(media_vector)\n",
    "    else:\n",
    "        atributo[\"tweet_vector\"].append([])\n",
    "atributo=pandas.DataFrame(atributo)\n",
    "\n",
    "objetivo=tweet_fifa_sent[\"sentimiento\"]\n",
    "\n",
    "(atributos_entrenamiento, atributos_prueba,\n",
    " objetivo_entrenamiento, objetivo_prueba) = train_test_split(\n",
    "       atributo, objetivo,\n",
    "       random_state=12345,\n",
    "       test_size=.2,\n",
    "       stratify=objetivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo desarrollado es 71.00998890122086\n"
     ]
    }
   ],
   "source": [
    "atributos_1=tweet_fifa_sent[\"tweet\"]\n",
    "vectorizer = CountVectorizer()\n",
    "atributos_1 = vectorizer.fit_transform(atributos_1)\n",
    "\n",
    "#Extrae el objetivo (ya es numérico)\n",
    "objetivo_1= tweet_fifa_sent[\"sentimiento\"]\n",
    "#Separa conjunto de entrenamiento y de prueba\n",
    "(atributos_entrenamiento_1, atributos_prueba_1,\n",
    " objetivo_entrenamiento_1, objetivo_prueba_1) = train_test_split(\n",
    "        atributos_1, objetivo_1,\n",
    "        random_state=12345,\n",
    "        test_size=.2,\n",
    "        stratify=objetivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RANDOM FOREST\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Entrenar el modelo Random Forest\n",
    "rf_classifier = RandomForestClassifier()\n",
    "rf_classifier.fit(list(atributos_entrenamiento[\"tweet_vector\"]), objetivo_entrenamiento)\n",
    "\n",
    "# Realizar predicciones con el conjunto de prueba\n",
    "predicciones_rf = rf_classifier.predict(list(atributos_prueba[\"tweet_vector\"]))\n",
    "\n",
    "# Calcular la precisión del modelo\n",
    "precision_rf = np.mean(predicciones_rf == objetivo_prueba)\n",
    "print(\"La precisión del modelo Random Forest es\", precision_rf * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### NAIVE BAYES\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Entrena el modelo de Naive Bayes usando la instancia MultinomialNB que es recomendada \n",
    "#para este tipo de tareas\n",
    "emotion_detector = MultinomialNB(alpha=1.0)  # alpha es el parámetro de suavizado\n",
    "emotion_detector.fit(atributos_entrenamiento_1, objetivo_entrenamiento_1)\n",
    "\n",
    "#Realiza las predicciones con el conjunto de prueba\n",
    "predicciones = emotion_detector.predict(atributos_prueba_1)\n",
    "#Calcular la precisión del modelo\n",
    "precision = emotion_detector.score(atributos_prueba_1, objetivo_prueba_1)\n",
    "print(\"La precisión del modelo Naive Bayes desarrollado es\", precision*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "name": "Solución_01_nueva.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
