{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Proyecto de análisis de sentimientos con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Carmen Giles Floriano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se realizará un modelo de aprendizaje automático capaz de analizar tweets y predecir el sentimiento del usuario dentro de las siguientes categorías: \"Muy feliz\", \"Contento\", \"Neutro\", \"Molesto\" y \"Hater\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ejercicio 1. RECOPILACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se lee el fichero `fifa_tweets_emotion.csv` mediante el paquete pandas. Los datos fueron obtenidos de Kaggle y son un conjunto de tweets relacionados con la WorldCup del FIFA 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22525, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>number_likes</th>\n",
       "      <th>source</th>\n",
       "      <th>tweet</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Date Created</td>\n",
       "      <td>Number of Likes</td>\n",
       "      <td>Source of Tweet</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-11-20 23:59:21+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-20 23:59:01+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-11-20 23:58:41+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-11-20 23:58:33+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number                       date     number_likes              source  \\\n",
       "0     NaN               Date Created  Number of Likes     Source of Tweet   \n",
       "1     0.0  2022-11-20 23:59:21+00:00                4     Twitter Web App   \n",
       "2     1.0  2022-11-20 23:59:01+00:00                3  Twitter for iPhone   \n",
       "3     2.0  2022-11-20 23:58:41+00:00                1  Twitter for iPhone   \n",
       "4     3.0  2022-11-20 23:58:33+00:00                1     Twitter Web App   \n",
       "\n",
       "                                               tweet    feeling  \n",
       "0                                              Tweet  Sentiment  \n",
       "1  What are we drinking today @TucanTribe \\n@MadB...    neutral  \n",
       "2  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   positive  \n",
       "3  Worth reading while watching #WorldCup2022 htt...   positive  \n",
       "4  Golden Maknae shinning bright\\n\\nhttps://t.co/...   positive  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con la función \"read_csv\" del paquete \"pandas\", leemos el data set indicando los nombres de las columnas.\n",
    "tweet_fifa_prev = pandas.read_csv('fifa_tweets_emotion.csv', header=None,\n",
    "                       names=['number', 'date', 'number_likes','source', 'tweet','feeling'])\n",
    "print(tweet_fifa_prev.shape)\n",
    "tweet_fifa_prev.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El data set presenta 22525 tweets distintos, cada uno representado en una fila. Además, del texto del propio tweet existen otras columnas con más información, como la fecha de publicación, el número de likes de la publicación, el lugar desde dónde se publicó y el sentimiento asociado. \n",
    "\n",
    "En este proyecto, sólo nos interesa el texto correspondiente al tweet. Los demás datos son eliminados, incluso la columna correspondiente a los sentimientos, ya que estos serán asignados siguiendo el criterio de la herramienta TextBlob. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "1  What are we drinking today @TucanTribe \\n@MadB...\n",
       "2  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...\n",
       "3  Worth reading while watching #WorldCup2022 htt...\n",
       "4  Golden Maknae shinning bright\\n\\nhttps://t.co/...\n",
       "5  If the BBC cares so much about human rights, h..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con la función \"drop\" se eliminan elementos del data set.\n",
    "## En la lista \"delete_col\" añadimos los nombres de las columnas que deseamos eliminar, la cuál será un parámetro de la función\n",
    "## \"drop\". Además, a esta función se le da el parámetro axis=1, lo que indica que los elementos a eliminar son columnas.\n",
    "\n",
    "delete_col=[\"number\", \"date\", \"number_likes\", \"source\", \"feeling\"]\n",
    "tweet_fifa_col= tweet_fifa_prev.drop(delete_col, axis=1)\n",
    "\n",
    "## Por otro lado, eliminamos la primera fila que contiene los anteriores nombres de las columnas del data set. Se elimina \n",
    "## indicando 0 (primer elemento) y el parámetro axis=0 (fila). \n",
    "\n",
    "tweet_fifa=tweet_fifa_col.drop(0, axis=0)\n",
    "tweet_fifa.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2. LIMPIEZA DEL TEXTO, ELIMINAR LAS PALABRAS QUE NO APORTAN INFORMACIÓN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, procesamos el texto del tweet de forma que sea más fácil de codificar y analizar en pasos posteriores. Para ello se crea la función \"limpiar_texto\", que elimina elementos como menciones, hashtags, URLs y emoticonos; convierte el texto en minúscula; elimina las palabras poco informativas; y, además, realizará una lematización, es decir, transforma las palabras a su forma base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recibe como entrada un conjunto de datos (data set).\n",
    "def limpiar_texto(data_set):\n",
    "    \n",
    "    ## En primer lugar, se cargan una serie de datos necesarios para el posterior procesamiento:\n",
    "    \n",
    "    # Se cargan los datos que hacen referencia a distintos emoticonos, utilizando \"re\", un módulo de la biblioteca de Pyhton. \n",
    "    # En concreto, con la funcion \"compile\" se almacena dicha información indicando el código Unicode del bloque de emoticonos\n",
    "    # correspondiente. El parámetro \"flags=re.UNICODE\" indica que la información añadida se lea según las reglas de Unicode, \n",
    "    # para que sean identificados como emoticonos.\n",
    "    patron_emoticonos = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # Emoticonos generales\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # Símbolos y pictogramas\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # Transporte y mapas\n",
    "                            u\"\\U0001F780-\\U0001F7FF\"  # Formas geométricas extendidas\n",
    "                            u\"\\U0001F800-\\U0001F8FF\"  # Signos de puntuación\n",
    "                            u\"\\U00002702-\\U000027B0\"  # Símbolos diversos, como tijeras o meteorológicos\n",
    "                            u\"\\U000024C2-\\U0001F251\"  # Transporte, pictogramas y otros símbolos\n",
    "                            u\"\\U0001F900-\\U0001F9FF\"  # Emoticonos de personas y cuerpos\n",
    "                            u\"\\U0001FA00-\\U0001FA6F\"  # Símbolos de objetos\n",
    "                            u\"\\U0001FA70-\\U0001FAFF\"  # Símbolos de alimentos\n",
    "                            u\"\\U0001F1E6-\\U0001F1FF\"  # Banderas \n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    # Se carga la información de la función \"TweetTokenize\" del paquete \"nltk\". \n",
    "    tokenizer=TweetTokenizer()\n",
    "    \n",
    "    # Se carga el conjunto de palabras \"stopwords\" del paquete nltk, estas son palabras muy frecuentes en el lenguaje pero poco\n",
    "    # informativas. \n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords_english = stopwords.words(\"english\") # Se selecciona el idioma inglés.\n",
    "    \n",
    "    # Se carga la información necesaria para la lematización mediante la función \"SnowballStemmer\" del paquete \"nltk\".\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    ## Se inicia un diccionario que alamacenará los tweets procesados.\n",
    "    diccionario = {\"tweet\": []}\n",
    "    \n",
    "    ## Para cada valor de i dentro del rango (0, longitud del data set): \n",
    "    for i in range(0, len(data_set)):\n",
    "        \n",
    "        ## Extraemos la fila número i, al trabajar con data frame es necesario utilizar el atributo \"iloc\". Además, se debe\n",
    "        ## seleccionar \"tweet\", para quedarnos sólo con la información de esa columna, es decir, el texto del tweet \n",
    "        ## correspondiente.\n",
    "        tweet=data_set.iloc[i][\"tweet\"]\n",
    "        \n",
    "        ## Con la función \"sub\" del módulo \"re\" se eliminan una serie de elementos, indicados en el primer parámetro de la \n",
    "        ## función (r'elemento'). \n",
    "        \n",
    "        # Se eliminan los hashtags (#)\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        \n",
    "        # Se eliminan las menciones, las cuáles tienen la siguiente estructura: @usuario. Al añadir \\S+, se indica que también \n",
    "        # se elimina cualquier texto que esté inmediatamente después del elemento determinado, sin incluir espacios.\n",
    "        tweet = re.sub(r'@\\S+', '', tweet)\n",
    "        \n",
    "        # Se eliminan los URLs, que por lo general comienzan con http.\n",
    "        tweet = re.sub(r'http\\S+', '', tweet) \n",
    "        \n",
    "        # Se eliminan los emoticonos usando el conjunto de emoticonos anteriormente guardados.\n",
    "        tweet = patron_emoticonos.sub(r'',tweet)\n",
    "        \n",
    "        ## Se transforma todo el texto a minúscula.\n",
    "        tweet=tweet.lower()\n",
    "        \n",
    "        ## Posteriormente, se procesará el texto palabra por palabra. Para ello, en primer lugar, se debe tokenizar el texto, \n",
    "        ## es decir, separar las palabras a elementos de una lista. Se realiza mediante la función \"TweetTokenizer\" cargada \n",
    "        ## anteriormente.\n",
    "        tweet= tokenizer.tokenize(tweet)\n",
    "        ## Se inicia una lista que contendrá las palabras procesadas del tweet.\n",
    "        list_tweet=[]\n",
    "\n",
    "        ## Para cada palabra:\n",
    "        for palabra in tweet:\n",
    "            ## Si la palabra NO está en el conjunto \"stopwords_english\", palabras con poco valor informativo:\n",
    "            if palabra not in stopwords_english:\n",
    "                ## Se transforma a su forma base con la función \"SnowballStemmer\".\n",
    "                palabra_proc = stemmer.stem(palabra)\n",
    "                ## La palabra ya procesada se añade a la lista anteriormente creada.\n",
    "                list_tweet.append(palabra_proc)\n",
    "        \n",
    "        ## Una vez procesadas las palabras del tweet, se vuelven a unir en la cadena que se inicia a continuación.\n",
    "        cadena_tweet=\"\"\n",
    "        \n",
    "        ## Para cada palabra:\n",
    "        for palabra in list_tweet:\n",
    "            ## Se añade cada palabra a la cadena anteriormente creada, separadas por espacios.\n",
    "            cadena_tweet+=\" \"\n",
    "            cadena_tweet+=palabra\n",
    "        \n",
    "        ## El tweet procesado se añade al diccionario    \n",
    "        diccionario[\"tweet\"].append(cadena_tweet)\n",
    "    \n",
    "    ## Se convierte el diccionario en un data set, mediante el paquete \"pandas\", y se devuelve.\n",
    "    data_set_proc=pandas.DataFrame(diccionario)\n",
    "    return data_set_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0                          drink today worldcup 2022\n",
       "1   amaz worldcup 2022 launch video . show much f...\n",
       "2                     worth read watch worldcup 2022\n",
       "3   golden makna shin bright jeonjungkook jungkoo...\n",
       "4   bbc care much human right , homosexu right , ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fifa_limp=limpiar_texto(tweet_fifa)\n",
    "tweet_fifa_limp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se obtienen un data set con tweets libres de emoticonos, de palabras pocas informativas y de otros elementos que no interesan a la hora de analizar el sentimiento de un tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Ejercicio 3. ETIQUETADO DE DATOS CON HERRAMIENTAS YA EXISTENTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se creará una función denominada \"clasificador\" con la cuál se asignará un sentimiento a cada tweet. Esto se realizará con el modelo \"TextBlob\" ya existente. Este asigna una puntuación (polaridad) a un texto dado, desde -1 a 1. Una puntuación cercana a -1 hace referencia a un texto con un sentimiento muy negativo y, cercana a 1, muy positivo. De esta forma, se asignan los sentimientos de la siguiente forma:\n",
    "\n",
    "Muy feliz: polaridad=[-1,-0.6)\n",
    "\n",
    "Contento: polaridad=[-0.6, -0.2)\n",
    "\n",
    "Neutro: polaridad=[-0.2, 0.2)\n",
    "\n",
    "Molesto: polaridad=[0.2, 0.6)\n",
    "\n",
    "Hater: polaridad=[0.6, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Como entrada la función recibe un data set.\n",
    "def clasificador(data_set):\n",
    "    \n",
    "    ## Se inicia un diccionario vacio, para guardar, por un lado, el tweet y, por otro lado, el sentimiento asociado.\n",
    "    diccionario={\"tweet\":[], \"sentimiento\":[]}\n",
    "    \n",
    "    ## Para cada valor de i dentro del rango (0, longitud del data set): \n",
    "    for i in range(0, len(data_set)):\n",
    "        \n",
    "        ## Extraemos la fila número i y seleccionamos la columna \"tweet\", para quedarnos sólo con el texto del tweet \n",
    "        ## correspondiente.\n",
    "        tweet=data_set.iloc[i][\"tweet\"]\n",
    "        \n",
    "        ## Se analiza con el paquete TextBlob.\n",
    "        texto=TextBlob(tweet)\n",
    "        \n",
    "        ## Se extrae el valor de polaridad que, como se explicó anteriormente, está asociado a un sentimiento. Siguiendo el \n",
    "        ## criterio anteriormente marcado, dependiendo del valor de polaridad calculado se asigna un sentimiento.\n",
    "        sentimiento_pol=texto.sentiment.polarity\n",
    "        if sentimiento_pol < (-0.6):\n",
    "            sentimiento=\"Hater\"\n",
    "        elif (-0.6) <= sentimiento_pol < (-0.2):\n",
    "            sentimiento=\"Molesto\"\n",
    "        elif (-0.2) <= sentimiento_pol < 0.2:\n",
    "            sentimiento=\"Neutro\"\n",
    "        elif 0.2 <= sentimiento_pol < 0.6:\n",
    "            sentimiento=\"Contento\"\n",
    "        else:\n",
    "            sentimiento=\"Muy feliz\"\n",
    "        \n",
    "        ## El tweet se almacena en el diccionario.\n",
    "        diccionario[\"tweet\"].append(tweet)\n",
    "        \n",
    "        ## El sentimiento, guardado en una cadena de carácteres, se almacena en el diccionario, generando una lista de cadenas.\n",
    "        diccionario[\"sentimiento\"].append(sentimiento)\n",
    "    \n",
    "    ## El diccionario se convierte en un data set y se devuelve.\n",
    "    data_set_sent=pandas.DataFrame(diccionario)\n",
    "    return data_set_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentimiento\n",
       "0                          drink today worldcup 2022      Neutro\n",
       "1   amaz worldcup 2022 launch video . show much f...      Neutro\n",
       "2                     worth read watch worldcup 2022    Contento\n",
       "3   golden makna shin bright jeonjungkook jungkoo...    Contento\n",
       "4   bbc care much human right , homosexu right , ...      Neutro"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fifa_sent=clasificador(tweet_fifa_limp)\n",
    "tweet_fifa_sent.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, se obtiene un data set con dos columnas, una con los tweets limpios y otra con el sentimiento asociado a dicho tweet. Este conjunto de datos será codificado y usado para entrenar un modelo propio, que permita asignar dichos sentimientos a nuevos tweets de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4. CODIFICACIÓN DE LOS ATRIBUTOS Y OBJETIVOS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar un modelo de aprendizaje automático es necesario definir los atributos, conjunto de datos de entrada sobre los cuáles se harán las predicciones, y los objetivos, las clasficaciones o características que se van a asociar a dichos atributos. En este caso, los atributos serán el texto procesado de los distintos tweets y los objetivos serán los sentimientos que estos transmiten. \n",
    "\n",
    "Para definir el modelo se debe codificar los atributos, el texto, a lenguaje numérico. Para ello existen múltiples codificadores con distintas características. Por ejemplo, se tiene el codificador ***CountVectorizer***, el cuál se basa en realizar recuentos de las distintas palabras, sin tener en cuenta la semántica o la posición de estas. Por esa razón, se buscó otras opciones más apropiadas para analizar tweets, dónde el contexto de las palabras es importante. Se estudiará el codificador ***Word2Vec***, el cuál sí que tiene en cuenta la relación entre las palabras, esto es útil, por ejemplo, para poder procesar expresiones o frases hechas. Este modelo asignará a cada palabra un vector con unas dimensiones determinadas. El modelo se basa en que las palabras que se parecen estarán representadas por vectores cercanos en el espacio. A continuación, vemos como se realiza una codificación de este tipo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[-0.3348931, 0.085357465, 0.16856542, -0.3086...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[-0.53843844, -0.016203731, 0.039964367, 0.05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[-0.32125136, 0.17830458, 0.14174609, -0.1361...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[-0.5124808, 0.4846268, 0.3248123, -0.3027562...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[-0.20676364, -0.0024545123, 0.43231094, 0.21...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tweet_vector\n",
       "0  [[-0.3348931, 0.085357465, 0.16856542, -0.3086...\n",
       "1  [[-0.53843844, -0.016203731, 0.039964367, 0.05...\n",
       "2  [[-0.32125136, 0.17830458, 0.14174609, -0.1361...\n",
       "3  [[-0.5124808, 0.4846268, 0.3248123, -0.3027562...\n",
       "4  [[-0.20676364, -0.0024545123, 0.43231094, 0.21..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Se obtiene la lista de tweets procesados.\n",
    "tweets=tweet_fifa_sent[\"tweet\"]\n",
    "\n",
    "## Este codificador necesita que el texto este tokenizado, es decir, que cada palabra sea un elemento de una lista. Esto se \n",
    "## aplica para cada tweet, mediante la función \"TweetTokenizer\" del paquete \"nltk\", y , posteriormente, los tweets tokenizados\n",
    "## se almacenan en una lista.\n",
    "tokens_list=[]\n",
    "tokenizer=TweetTokenizer()\n",
    "for tweet in tweets:\n",
    "    tokens= tokenizer.tokenize(tweet)\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "\n",
    "## Se entrena el modelo de codificación Word2Vec, utilizando la lista de tokens previamente generada. El parámetro \n",
    "## \"vector_size\" indica el número de dimensiones que posee el vector que representa cada palabra; por otro lado, \"windows\" \n",
    "## indica el número de palabras a cada lado de la analizada que se consideran para entender el contexto; y \"min_count\" indica \n",
    "## el número de veces que dicha palabra debe aparecer en el conjunto de datos para formar parte del modelo. Estos parámetros han\n",
    "## sido ajustados para obtener un mejor funcionamiento del modelo.\n",
    "model = Word2Vec(tokens_list, vector_size=100, window=7, min_count=5)\n",
    "\n",
    "## Se inicia un diccionario que contendrá la codificación de los atributos.\n",
    "atributo={\"tweet_vector\":[]}\n",
    "\n",
    "## Para cada tweet tokenizado:\n",
    "for tweet in tokens_list:\n",
    "    lista=[]\n",
    "    ## Para cada palabra de dicho tweet:\n",
    "    for palabra in tweet:\n",
    "        ## Si la palabra está presente en el modelo previamente generado:\n",
    "        if palabra in model.wv:\n",
    "            ## Se realiza la codificación de dicha palabra, generando un vector que se añade a la lista anteriormente creada.\n",
    "            codificacion = model.wv[palabra]\n",
    "            lista.append(codificacion)\n",
    "    ## Si no está la lista vacia:\n",
    "    if lista:\n",
    "        ## Se calcula el promedio de todos los vectores para tener una representación única del tweet.\n",
    "        media_vector=sum(lista)/len(lista)\n",
    "        ## Dicha media se añade al diccionario \"atributos\".\n",
    "        atributo[\"tweet_vector\"].append(media_vector)\n",
    "    ## Si la lista está vacia:\n",
    "    else:\n",
    "        ## Se añade un elemento vacio a la lista.\n",
    "        atributo[\"tweet_vector\"].append([])\n",
    "\n",
    "## El diccionario se convierte en un data set.\n",
    "atributo=pandas.DataFrame(atributo)\n",
    "\n",
    "atributo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que cada tweet, está representado por un vector. Como se ha mencionado anteriormente, este vector es la media de los correspondientes a cada palabra; esto es necesario para que sea compatible con el algoritmo usado para generar le modelo. Una disposición semejante en el espacio de los distintos vectores, indica una relación semántica.\n",
    "\n",
    "Una vez los atributos han sido codificados, se dividen los tweets en aquellos que serán usados en el entrenamiento del modelo y los que serán usados en una prueba de precisión. Para ello se usa la función *train_test_split* del paquete *scikit-learn*, cargada a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dividir el conjunto de entrenamiento y el de prueba, se deben incluir los siguientes parámetros: \n",
    "\n",
    "*random_state*: es una especie de código numérico, que garantiza que cada vez que pongas el mismo número, se producirá la misma división. \n",
    "\n",
    "*test_size*: porcentaje del conjunto de prueba en comparación con el conjunto de entrenamiento. En este caso, el 20% de los datos serán para la prueba.\n",
    "\n",
    "*stratify*: necesario para asegurar que la proporción de objetivos presentes en el conjunto de prueba sea el mismo que el conjunto total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se extrae el valor de la columna \"sentimientos\" del data set a estudiar. Este conjunto de datos serán los objetivos.\n",
    "objetivo=tweet_fifa_sent[\"sentimiento\"]\n",
    "\n",
    "## Se dividen los atributos y los objetivos, en conjuntos de prueba y entrenamiento, como se explicó anteriormente.\n",
    "(atributos_entrenamiento, atributos_prueba,\n",
    " objetivo_entrenamiento, objetivo_prueba) = train_test_split(\n",
    "       atributo, objetivo,\n",
    "       random_state=12345,\n",
    "       test_size=.2,\n",
    "       stratify=objetivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5: ENTRENAMIENTO DEL MODELO\n",
    "\n",
    "Finalmente, después de tener codificados los atributos y separado el conjunto de datos para el entrenamieto y la prueba, se genera el modelo. Para ello, se realiza el entrenamiento, que implica proporcionar un conjunto de atributos junto con sus objetivos correspondientes. De este modo, el modelo se ajusta y será capaz de generalizar a partir de estos datos específicos, pudiendo predecir los objetivos de otros atributos. \n",
    "\n",
    "Al estar trabajando con texto codificado como vectores, generado mediante *Word2Vec*, se necesita realizar un modelo compatible, en concreto se eligió ***Random Forest***. Este algoritmo, en realidad, está formado por un conjunto de modelos individuales que se combinan para obtener un modelo más preciso; lo que supone una ventaja extra. \n",
    "\n",
    "Para ello, se importa *RandomForestClassifier* del módulo *ensemble* del paquete *scikit-learn*, anteriormente también utilizado para la codificación. Además, se carga el paquete numpy, utilizado para calcular la predicción del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conituación, se genera el modelo usando *Random Forest* y los conjuntos de entrenamiento anteriormente extraidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (18019,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m random_forest \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m## Como el conjunto de atributos de entrenamiento es un data frame, es necesario indicar la columna que se desea analizar.\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m random_forest\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;28mlist\u001b[39m(atributos_entrenamiento[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m]), objetivo_entrenamiento)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m## Una vez generado el modelo se obtienen las predicciones del conjunto de prueba; usando la función \"predict\".\u001b[39;00m\n\u001b[0;32m      7\u001b[0m predicciones_rf \u001b[38;5;241m=\u001b[39m rf_classifier\u001b[38;5;241m.\u001b[39mpredict(\u001b[38;5;28mlist\u001b[39m(atributos_prueba[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtweet_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m]))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:348\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    347\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 348\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    349\u001b[0m     X, y, multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39mDTYPE\n\u001b[0;32m    350\u001b[0m )\n\u001b[0;32m    351\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:621\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    619\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    620\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 621\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    622\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    624\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1147\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1144\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1145\u001b[0m     )\n\u001b[1;32m-> 1147\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1148\u001b[0m     X,\n\u001b[0;32m   1149\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1150\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1151\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1152\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1153\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1154\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1155\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1156\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1157\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1158\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1159\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1160\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1161\u001b[0m )\n\u001b[0;32m   1163\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1165\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:917\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    915\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    916\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 917\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    920\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    921\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:380\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    378\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 380\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (18019,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "## Se entrena el modelo Random Forest, con RandomForestClassifier y su función \"fit\".\n",
    "random_forest = RandomForestClassifier()\n",
    "## Como el conjunto de atributos de entrenamiento es un data frame, es necesario indicar la columna que se desea analizar.\n",
    "random_forest.fit(list(atributos_entrenamiento[\"tweet_vector\"]), objetivo_entrenamiento)\n",
    "\n",
    "## Una vez generado el modelo se obtienen las predicciones del conjunto de prueba; usando la función \"predict\".\n",
    "predicciones_rf = rf_classifier.predict(list(atributos_prueba[\"tweet_vector\"]))\n",
    "\n",
    "## Se calcula la predicción del modelo. Para ello, se cuenta el número de veces que las predicciones generadas son equivalentes \n",
    "## a los objetivos reales y se normaliza, usando el paquete numpy.\n",
    "precision_rf = numpy.mean(predicciones_rf == objetivo_prueba)\n",
    "print(\"La precisión del modelo es\", precision_rf * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del data set procesado y limpio, usando *Word2Vec* como codificador y *Random Forest*, se ha conseguido obtener un modelo con un **71.85% de predicción**. Para aumentar este valor, se podría entrenar el modelo con un conjunto de datos más grande. Sin embargo, se trata de una predicción alta y suficiente para dar por válido el modelo. \n",
    "\n",
    "Por otro lado, se quiso comparar la eficiencia de este modelo con otro generado con ***Naive Bayes***. Este es un algoritmo de aprendizaje basado en el teorema de Bayes, utiliza las probabilidades para realizar la clasificación del texto. Este modelo espera recibir como entrada datos unidimensionales, por tanto, no se puede usar los atributos codificados con *Word2Vec*, ya que los datos se han expresado como vectores de 100 dimensiones. Por esta razón, se usará ***CountVectorizer*** para codificar el texto, el cuál generará una matriz de recuento de términos que será compatible con el modelo *Naive Bayes*. \n",
    "\n",
    "Se importa *CountVectorizer* del paquete *scikit-learn*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentimiento\n",
       "0                          drink today worldcup 2022      Neutro\n",
       "1   amaz worldcup 2022 launch video . show much f...      Neutro\n",
       "2                     worth read watch worldcup 2022    Contento\n",
       "3   golden makna shin bright jeonjungkook jungkoo...    Contento\n",
       "4   bbc care much human right , homosexu right , ...      Neutro"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se codifican los atributos con *CountVectorizer* y, tal y como se hizo anteriormente, se generan el conjunto de entrenamiento y el de prueba, mediante la función *train_test_split*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se extraen los atributos, la columna de tweets del data frame \"tweet_fifa_sent\".\n",
    "atributos=tweet_fifa_sent[\"tweet\"]\n",
    "\n",
    "## Se codifican dichos atributos.\n",
    "vectorizer = CountVectorizer()\n",
    "atributos_1 = vectorizer.fit_transform(atributos)\n",
    "\n",
    "## Con la función \"train_test_split\" y los parámetros anteriormente usados, se generan el conjunto de prueba y el de \n",
    "## entrenamiento. Se usan los atributos codificados con \"CountVectorizer\" y los objetivos anteriormente extraidos. \n",
    "(atributos_entrenamiento_1, atributos_prueba_1,\n",
    " objetivo_entrenamiento_1, objetivo_prueba_1) = train_test_split(\n",
    "        atributos_1, objetivo,\n",
    "        random_state=12345,\n",
    "        test_size=.2,\n",
    "        stratify=objetivo)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez codificado los atributos y separado el conjunto de entrenamiento del de prueba, se genera el modelo con *Naive Bayes*. Para ello, se carga la instancia *MultinomialNB*, de nuevo del paquete *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se entrena el modelo de *Naive Bayes* y se calcula la predicción de este."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo Naive Bayes desarrollado es 71.03218645948945\n"
     ]
    }
   ],
   "source": [
    "## Se entrena usando \"MultinomialNB\", la función \"fit\" y el conjunto de entrenamiento.\n",
    "emotion_detector = MultinomialNB(alpha=1.0)  \n",
    "emotion_detector.fit(atributos_entrenamiento_1, objetivo_entrenamiento_1)\n",
    "\n",
    "## Se calcula la precisión del modelo con la función score.\n",
    "precision = emotion_detector.score(atributos_prueba_1, objetivo_prueba_1)\n",
    "print(\"La precisión del modelo desarrollado es\", precision*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partiendo del mismo data set usado para el anterior modelo, pero esta vez utilizando *CountVectorizer* como codificador y *Naive Bayes* para realizar el modelo, se obtiene una precisión ligeramente inferior, en concreto 71.03%. Como se comentó anteriormente, el modelo generado con *Word2Vec* y *Random Forest* tiene la ventaja de que interpreta el contexto de las palabras y, además, utiliza múltiples modelos. Debido a esto, inicialmente se pensó que este método era mejor para analizar sentimientos de los tweets.  Aunque se observa que efectivamente tiene una precisión superior, es decir, el modelo obtenido usando *Random Forest* es superior que el obtenido usando *Naive Bayes*; la diferencia es muy pequeña. Debido a esto, se probará el análisis del siguiente apartado con los dos modelos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 6: USAR EL MODELO ENTRENADO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>None</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Make me the manager. I'll do 100% better then ...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Richards &gt; Kelly Rio &gt; Kelly Smalling &gt; Kelly</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Martin Kelly Martin Kelly? ARE YOU FUCKING SER...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Martin Kelly called up as replacement for Cahi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hate every single Liverpool player but hate LU...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet None\n",
       "0  Make me the manager. I'll do 100% better then ...     \n",
       "1     Richards > Kelly Rio > Kelly Smalling > Kelly      \n",
       "2  Martin Kelly Martin Kelly? ARE YOU FUCKING SER...  NaN\n",
       "3  Martin Kelly called up as replacement for Cahi...     \n",
       "4  Hate every single Liverpool player but hate LU...     "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hater_prev = pandas.read_csv('liverpool_hater.csv', header=None, names=[\"tweet\", \"None\"])\n",
    "hater_prev.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>make manag . i'll 100 % better roy see liverp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>richard &gt; kelli rio &gt; kelli small &gt; kelli</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>martin kelli martin kelli ? fuck serious ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>martin kelli call replac cahil . england even...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hate everi singl liverpool player hate lui sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0   make manag . i'll 100 % better roy see liverp...\n",
       "1          richard > kelli rio > kelli small > kelli\n",
       "2         martin kelli martin kelli ? fuck serious ?\n",
       "3   martin kelli call replac cahil . england even...\n",
       "4   hate everi singl liverpool player hate lui sa..."
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hater=hater_prev.drop(\"None\", axis=1)\n",
    "hater=limpiar_texto(hater)\n",
    "hater.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Realiza las predicciones con el conjunto de prueba\n",
    "#predicciones = emotion_detector.predict(atributos_prueba)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "name": "Solución_01_nueva.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
