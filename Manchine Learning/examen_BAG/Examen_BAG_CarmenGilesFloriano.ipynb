{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Proyecto de análisis de sentimientos con Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Carmen Giles Floriano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este proyecto se realizará un modelo de aprendizaje automático capaz de analizar tweets y predecir el sentimiento del usuario dentro de las siguientes categorías: \"Muy feliz\", \"Contento\", \"Neutro\", \"Molesto\" y \"Hater\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Ejercicio 1. RECOPILACIÓN DE DATOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas \n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se lee el fichero `fifa_tweets_emotion.csv` mediante el paquete pandas. Los datos fueron obtenidos de Kaggle y son un conjunto de tweets relacionados con la WorldCup del FIFA 2022. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22525, 6)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>date</th>\n",
       "      <th>number_likes</th>\n",
       "      <th>source</th>\n",
       "      <th>tweet</th>\n",
       "      <th>feeling</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Date Created</td>\n",
       "      <td>Number of Likes</td>\n",
       "      <td>Source of Tweet</td>\n",
       "      <td>Tweet</td>\n",
       "      <td>Sentiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-11-20 23:59:21+00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2022-11-20 23:59:01+00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>2022-11-20 23:58:41+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>2022-11-20 23:58:33+00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Twitter Web App</td>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number                       date     number_likes              source  \\\n",
       "0     NaN               Date Created  Number of Likes     Source of Tweet   \n",
       "1     0.0  2022-11-20 23:59:21+00:00                4     Twitter Web App   \n",
       "2     1.0  2022-11-20 23:59:01+00:00                3  Twitter for iPhone   \n",
       "3     2.0  2022-11-20 23:58:41+00:00                1  Twitter for iPhone   \n",
       "4     3.0  2022-11-20 23:58:33+00:00                1     Twitter Web App   \n",
       "\n",
       "                                               tweet    feeling  \n",
       "0                                              Tweet  Sentiment  \n",
       "1  What are we drinking today @TucanTribe \\n@MadB...    neutral  \n",
       "2  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...   positive  \n",
       "3  Worth reading while watching #WorldCup2022 htt...   positive  \n",
       "4  Golden Maknae shinning bright\\n\\nhttps://t.co/...   positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con la función \"read_csv\" del paquete \"pandas\", leemos el data set indicando los nombres de las columnas.\n",
    "tweet_fifa_prev = pandas.read_csv('fifa_tweets_emotion.csv', header=None,\n",
    "                       names=['number', 'date', 'number_likes','source', 'tweet','feeling'])\n",
    "print(tweet_fifa_prev.shape)\n",
    "tweet_fifa_prev.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El data set presenta 22525 tweets distintos, cada uno representado en una fila. Además, del texto del propio tweet existen otras columnas con más información, como la fecha de publicación, el número de likes de la publicación, el lugar desde dónde se publicó y el sentimiento asociado. \n",
    "\n",
    "En este proyecto, sólo nos interesa el texto correspondiente al tweet. Los demás datos son eliminados, incluso la columna correspondiente a los sentimientos, ya que estos serán asignados siguiendo el criterio de la herramienta TextBlob. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are we drinking today @TucanTribe \\n@MadB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amazing @CanadaSoccerEN  #WorldCup2022 launch ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Worth reading while watching #WorldCup2022 htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Golden Maknae shinning bright\\n\\nhttps://t.co/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>If the BBC cares so much about human rights, h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "1  What are we drinking today @TucanTribe \\n@MadB...\n",
       "2  Amazing @CanadaSoccerEN  #WorldCup2022 launch ...\n",
       "3  Worth reading while watching #WorldCup2022 htt...\n",
       "4  Golden Maknae shinning bright\\n\\nhttps://t.co/...\n",
       "5  If the BBC cares so much about human rights, h..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con la función \"drop\" se eliminan elementos del data set.\n",
    "## En la lista \"delete_col\" añadimos los nombres de las columnas que deseamos eliminar, la cuál será un parámetro de la función\n",
    "## \"drop\". Además, a esta función se le da el parámetro axis=1, lo que indica que los elementos a eliminar son columnas.\n",
    "\n",
    "delete_col=[\"number\", \"date\", \"number_likes\", \"source\", \"feeling\"]\n",
    "tweet_fifa_col= tweet_fifa_prev.drop(delete_col, axis=1)\n",
    "\n",
    "## Por otro lado, eliminamos la primera fila que contiene los anteriores nombres de las columnas del data set. Se elimina \n",
    "## indicando 0 (primer elemento) y el parámetro axis=0 (fila). \n",
    "\n",
    "tweet_fifa=tweet_fifa_col.drop(0, axis=0)\n",
    "tweet_fifa.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 2. LIMPIEZA DEL TEXTO, ELIMINAR LAS PALABRAS QUE NO APORTAN INFORMACIÓN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, procesamos el texto del tweet de forma que sea más fácil de codificar y analizar en pasos posteriores. Para ello se crea la función \"limpiar_texto\", que elimina elementos como menciones, hashtags, URLs y emoticonos; convierte el texto en minúscula; elimina las palabras poco informativas; y, además, realizará una lematización, es decir, transforma las palabras a su forma base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recibe como entrada un conjunto de datos (data set).\n",
    "def limpiar_texto(data_set):\n",
    "    \n",
    "    ## En primer lugar, se cargan una serie de datos necesarios para el posterior procesamiento:\n",
    "    \n",
    "    # Se cargan los datos que hacen referencia a distintos emoticonos, utilizando \"re\", un módulo de la biblioteca de Pyhton. \n",
    "    # En concreto, con la funcion \"compile\" se almacena dicha información indicando el código Unicode del bloque de emoticonos\n",
    "    # correspondiente. El parámetro \"flags=re.UNICODE\" indica que la información añadida se lea según las reglas de Unicode, \n",
    "    # para que sean identificados como emoticonos.\n",
    "    patron_emoticonos = re.compile(\"[\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  # Emoticonos generales\n",
    "                            u\"\\U0001F300-\\U0001F5FF\"  # Símbolos y pictogramas\n",
    "                            u\"\\U0001F680-\\U0001F6FF\"  # Transporte y mapas\n",
    "                            u\"\\U0001F780-\\U0001F7FF\"  # Formas geométricas extendidas\n",
    "                            u\"\\U0001F800-\\U0001F8FF\"  # Signos de puntuación\n",
    "                            u\"\\U00002702-\\U000027B0\"  # Símbolos diversos, como tijeras o meteorológicos\n",
    "                            u\"\\U000024C2-\\U0001F251\"  # Transporte, pictogramas y otros símbolos\n",
    "                            u\"\\U0001F900-\\U0001F9FF\"  # Emoticonos de personas y cuerpos\n",
    "                            u\"\\U0001FA00-\\U0001FA6F\"  # Símbolos de objetos\n",
    "                            u\"\\U0001FA70-\\U0001FAFF\"  # Símbolos de alimentos\n",
    "                            u\"\\U0001F1E6-\\U0001F1FF\"  # Banderas \n",
    "                            \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    # Se carga la información de la función \"TweetTokenize\" del paquete \"nltk\". \n",
    "    tokenizer=TweetTokenizer()\n",
    "    \n",
    "    # Se carga el conjunto de palabras \"stopwords\" del paquete nltk, estas son palabras muy frecuentes en el lenguaje pero poco\n",
    "    # informativas. \n",
    "    nltk.download(\"stopwords\")\n",
    "    stopwords_english = stopwords.words(\"english\") # Se selecciona el idioma inglés.\n",
    "    \n",
    "    # Se carga la información necesaria para la lematización mediante la función \"SnowballStemmer\" del paquete \"nltk\".\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    \n",
    "    ## Se inicia un diccionario que alamacenará los tweets procesados.\n",
    "    diccionario = {\"tweet\": []}\n",
    "    \n",
    "    ## Para cada valor de i dentro del rango (0, longitud del data set): \n",
    "    for i in range(0, len(data_set)):\n",
    "        \n",
    "        ## Extraemos la fila número i, al trabajar con data frame es necesario utilizar el atributo \"iloc\". Además, se debe\n",
    "        ## seleccionar \"tweet\", para quedarnos sólo con la información de esa columna, es decir, el texto del tweet \n",
    "        ## correspondiente.\n",
    "        tweet=data_set.iloc[i][\"tweet\"]\n",
    "        \n",
    "        ## Con la función \"sub\" del módulo \"re\" se eliminan una serie de elementos, indicados en el primer parámetro de la \n",
    "        ## función (r'elemento'). \n",
    "        \n",
    "        # Se eliminan los hashtags (#)\n",
    "        tweet = re.sub(r'#', '', tweet)\n",
    "        \n",
    "        # Se eliminan las menciones, las cuáles tienen la siguiente estructura: @usuario. Al añadir \\S+, se indica que también \n",
    "        # se elimina cualquier texto que esté inmediatamente después del elemento determinado, sin incluir espacios.\n",
    "        tweet = re.sub(r'@\\S+', '', tweet)\n",
    "        \n",
    "        # Se eliminan los URLs, que por lo general comienzan con http.\n",
    "        tweet = re.sub(r'http\\S+', '', tweet) \n",
    "        \n",
    "        # Se eliminan los emoticonos usando el conjunto de emoticonos anteriormente guardados.\n",
    "        tweet = patron_emoticonos.sub(r'',tweet)\n",
    "        \n",
    "        ## Se transforma todo el texto a minúscula.\n",
    "        tweet=tweet.lower()\n",
    "        \n",
    "        ## Posteriormente, se procesará el texto palabra por palabra. Para ello, en primer lugar, se debe tokenizar el texto, \n",
    "        ## es decir, separar las palabras a elementos de una lista. Se realiza mediante la función \"TweetTokenizer\" cargada \n",
    "        ## anteriormente.\n",
    "        tweet= tokenizer.tokenize(tweet)\n",
    "        ## Se inicia una lista que contendrá las palabras procesadas del tweet.\n",
    "        list_tweet=[]\n",
    "\n",
    "        ## Para cada palabra:\n",
    "        for palabra in tweet:\n",
    "            ## Si la palabra NO está en el conjunto \"stopwords_english\", palabras con poco valor informativo:\n",
    "            if palabra not in stopwords_english:\n",
    "                ## Se transforma a su forma base con la función \"SnowballStemmer\".\n",
    "                palabra_proc = stemmer.stem(palabra)\n",
    "                ## La palabra ya procesada se añade a la lista anteriormente creada.\n",
    "                list_tweet.append(palabra_proc)\n",
    "        \n",
    "        ## Una vez procesadas las palabras del tweet, se vuelven a unir en la cadena que se inicia a continuación.\n",
    "        cadena_tweet=\"\"\n",
    "        \n",
    "        ## Para cada palabra:\n",
    "        for palabra in list_tweet:\n",
    "            ## Se añade cada palabra a la cadena anteriormente creada, separadas por espacios.\n",
    "            cadena_tweet+=\" \"\n",
    "            cadena_tweet+=palabra\n",
    "        \n",
    "        ## El tweet procesado se añade al diccionario    \n",
    "        diccionario[\"tweet\"].append(cadena_tweet)\n",
    "    \n",
    "    ## Se convierte el diccionario en un data set, mediante el paquete \"pandas\", y se devuelve.\n",
    "    data_set_proc=pandas.DataFrame(diccionario)\n",
    "    return data_set_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet\n",
       "0                          drink today worldcup 2022\n",
       "1   amaz worldcup 2022 launch video . show much f...\n",
       "2                     worth read watch worldcup 2022\n",
       "3   golden makna shin bright jeonjungkook jungkoo...\n",
       "4   bbc care much human right , homosexu right , ..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fifa_limp=limpiar_texto(tweet_fifa)\n",
    "tweet_fifa_limp.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se obtienen un data set con tweets libres de emoticonos, de palabras pocas informativas y de otros elementos que no interesan a la hora de analizar el sentimiento de un tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "#### Ejercicio 3. ETIQUETADO DE DATOS CON HERRAMIENTAS YA EXISTENTES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se creará una función denominada \"clasificador\" con la cuál se asignará un sentimiento a cada tweet. Esto se realizará con el modelo \"TextBlob\" ya existente. Este asigna una puntuación (polaridad) a un texto dado, desde -1 a 1. Una puntuación cercana a -1 hace referencia a un texto con un sentimiento muy negativo y, cercana a 1, muy positivo. De esta forma, se asignan los sentimientos de la siguiente forma:\n",
    "\n",
    "Muy feliz: polaridad=[-1,-0.6)\n",
    "\n",
    "Contento: polaridad=[-0.6, -0.2)\n",
    "\n",
    "Neutro: polaridad=[-0.2, 0.2)\n",
    "\n",
    "Molesto: polaridad=[0.2, 0.6)\n",
    "\n",
    "Hater: polaridad=[0.6, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Como entrada la función recibe un data set.\n",
    "def clasificador(data_set):\n",
    "    \n",
    "    ## Se inicia un diccionario vacio, para guardar, por un lado, el tweet y, por otro lado, el sentimiento asociado.\n",
    "    diccionario={\"tweet\":[], \"sentimiento\":[]}\n",
    "    \n",
    "    ## Para cada valor de i dentro del rango (0, longitud del data set): \n",
    "    for i in range(0, len(data_set)):\n",
    "        \n",
    "        ## Extraemos la fila número i y seleccionamos la columna \"tweet\", para quedarnos sólo con el texto del tweet \n",
    "        ## correspondiente.\n",
    "        tweet=data_set.iloc[i][\"tweet\"]\n",
    "        \n",
    "        ## Se analiza con el paquete TextBlob.\n",
    "        texto=TextBlob(tweet)\n",
    "        \n",
    "        ## Se extrae el valor de polaridad que, como se explicó anteriormente, está asociado a un sentimiento. Siguiendo el \n",
    "        ## criterio anteriormente marcado, dependiendo del valor de polaridad calculado se asigna un sentimiento.\n",
    "        sentimiento_pol=texto.sentiment.polarity\n",
    "        if sentimiento_pol < (-0.6):\n",
    "            sentimiento=\"Hater\"\n",
    "        elif (-0.6) <= sentimiento_pol < (-0.2):\n",
    "            sentimiento=\"Molesto\"\n",
    "        elif (-0.2) <= sentimiento_pol < 0.2:\n",
    "            sentimiento=\"Neutro\"\n",
    "        elif 0.2 <= sentimiento_pol < 0.6:\n",
    "            sentimiento=\"Contento\"\n",
    "        else:\n",
    "            sentimiento=\"Muy feliz\"\n",
    "        \n",
    "        ## El tweet se almacena en el diccionario.\n",
    "        diccionario[\"tweet\"].append(tweet)\n",
    "        \n",
    "        ## El sentimiento, guardado en una cadena de carácteres, se almacena en el diccionario, generando una lista de cadenas.\n",
    "        diccionario[\"sentimiento\"].append(sentimiento)\n",
    "    \n",
    "    ## El diccionario se convierte en un data set y se devuelve.\n",
    "    data_set_sent=pandas.DataFrame(diccionario)\n",
    "    return data_set_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>sentimiento</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>drink today worldcup 2022</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>amaz worldcup 2022 launch video . show much f...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>worth read watch worldcup 2022</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>golden makna shin bright jeonjungkook jungkoo...</td>\n",
       "      <td>Contento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bbc care much human right , homosexu right , ...</td>\n",
       "      <td>Neutro</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet sentimiento\n",
       "0                          drink today worldcup 2022      Neutro\n",
       "1   amaz worldcup 2022 launch video . show much f...      Neutro\n",
       "2                     worth read watch worldcup 2022    Contento\n",
       "3   golden makna shin bright jeonjungkook jungkoo...    Contento\n",
       "4   bbc care much human right , homosexu right , ...      Neutro"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_fifa_sent=clasificador(tweet_fifa_limp)\n",
    "tweet_fifa_sent.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta forma, se obtiene un data set con dos columnas, una con los tweets limpios y otra con el sentimiento asociado a dicho tweet. Este conjunto de datos será codificado y usado para entrenar un modelo propio, que permita asignar dichos sentimientos a nuevos tweets de prueba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 4. CODIFICACIÓN DE LOS ATRIBUTOS Y OBJETIVOS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para realizar un modelo de aprendizaje automático es necesario definir los atributos, conjunto de datos de entrada sobre los cuáles se harán las predicciones, y los objetivos, las clasficaciones o características que se van a asociar a dichos atributos. En este caso, los atributos serán el texto procesado de los distintos tweets y los objetivos serán los sentimientos que estos transmiten. \n",
    "\n",
    "Para definir el modelo se debe codificar los atributos, el texto, a lenguaje numérico. Para ello existen múltiples codificadores con distintas características. Por ejemplo, se tiene el codificador ***CountVectorizer***, el cuál se basa en realizar recuentos de las distintas palabras, sin tener en cuenta la semántica o la posición de estas. Por esa razón, se buscó otras opciones más apropiadas para analizar tweets, dónde el contexto de las palabras es importante. Se estudiará el codificador ***Word2Vec***, el cuál sí que tiene en cuenta la relación entre las palabras, esto es útil, por ejemplo, para poder procesar expresiones o frases hechas. Este modelo asignará a cada palabra un vector con unas dimensiones determinadas. El modelo se basa en que las palabras que se parecen estarán representadas por vectores cercanos en el espacio. A continuación, vemos como se realiza una codificación de este tipo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USUARIO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[0.12690741, 0.17684227, 0.31365395, 0.3334574...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[0.037999928, 0.05471419, 0.35340443, -0.18842...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-0.02919066, 0.14123979, 0.23998518, 0.221679...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-0.67328936, 0.2685161, 0.060859684, 0.129616...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-0.28611276, 0.18276027, 0.3704295, 0.0857501...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        tweet_vector\n",
       "0  [0.12690741, 0.17684227, 0.31365395, 0.3334574...\n",
       "1  [0.037999928, 0.05471419, 0.35340443, -0.18842...\n",
       "2  [-0.02919066, 0.14123979, 0.23998518, 0.221679...\n",
       "3  [-0.67328936, 0.2685161, 0.060859684, 0.129616...\n",
       "4  [-0.28611276, 0.18276027, 0.3704295, 0.0857501..."
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Se obtiene la lista de tweets procesados.\n",
    "tweets=tweet_fifa_sent[\"tweet\"]\n",
    "\n",
    "## Este codificador necesita que el texto este tokenizado, es decir, que cada palabra sea un elemento de una lista. Esto se \n",
    "## aplica para cada tweet, mediante la función \"TweetTokenizer\" del paquete \"nltk\", y , posteriormente, los tweets tokenizados\n",
    "## se almacenan en una lista.\n",
    "tokens_list=[]\n",
    "tokenizer=TweetTokenizer()\n",
    "for tweet in tweets:\n",
    "    tokens= tokenizer.tokenize(tweet)\n",
    "    tokens_list.append(tokens)\n",
    "\n",
    "\n",
    "## Se entrena el modelo de codificación Word2Vec, utilizando la lista de tokens previamente generada. El parámetro \n",
    "## \"vector_size\" indica el número de dimensiones que posee el vector que representa cada palabra; por otro lado, \"windows\" \n",
    "## indica el número de palabras a cada lado de la analizada que se consideran para entender el contexto; y \"min_count\" indica \n",
    "## el número de veces que dicha palabra debe aparecer en el conjunto de datos para formar parte del modelo. \n",
    "model = Word2Vec(tokens_list, vector_size=100, window=7, min_count=5)\n",
    "\n",
    "## Se inicia un diccionario que contendrá la codificación para cada tweet.\n",
    "atributo={\"tweet_vector\":[]}\n",
    "\n",
    "## Para cada tweet tokenizado:\n",
    "for tweet in tokens_list:\n",
    "    lista=[]\n",
    "    ## Para cada palabra de dicho tweet:\n",
    "    for palabra in tweet:\n",
    "        ## Si la palabra está presente en el modelo previamente generado:\n",
    "        if palabra in model.wv:\n",
    "            ## Se realiza la codificación de dicha palabra, generando un vector que se añade a la lista anteriormente creada.\n",
    "            codificacion = model.wv[palabra]\n",
    "            lista.append(codificacion)\n",
    "    ## Si no está la lista vacia:\n",
    "    if lista:\n",
    "        ## Se calcula el promedio de todos los vectores para tener una representación única del tweet.\n",
    "        media_vector=sum(lista)/len(lista)\n",
    "        ## Dicha media se añade al diccionario \"atributos\".\n",
    "        atributo[\"tweet_vector\"].append(media_vector)\n",
    "    ## Si la lista está vacia:\n",
    "    else:\n",
    "        ## Se añade un elemento vacio a la lista.\n",
    "        atributo[\"tweet_vector\"].append([])\n",
    "\n",
    "## El diccionario se convierte en un data set.\n",
    "atributo=pandas.DataFrame(atributo)\n",
    "\n",
    "atributo.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez los atributos han sido codificados, se dividen los tweets en aquellos que serán usados en el entrenamiento del modelo y los que serán usados de prueba. Para ello se usa la función *train_test_split* del paquete *scikit-learn*, cargada a continuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dividir el conjunto de entrenamiento y el de prueba, se deben incluir los siguientes parámetros: \n",
    "\n",
    "*random_state*: es una especie de código numérico, que garantiza que cada vez que pongas el mismo número, se producirá la misma división. \n",
    "\n",
    "*test_size*: porcentaje del conjunto de prueba en comparación con el conjunto de entrenamiento. En este caso, el 20% de los datos serán para la prueba.\n",
    "\n",
    "*stratify*: necesario para asegurar que la proporción de objetivos presentes en el conjunto de prueba sea el mismo que el conjunto total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se extrae el valor de la columna \"sentimientos\" del data set a estudiar. Este conjunto de datos serán los objetivos.\n",
    "objetivo=tweet_fifa_sent[\"sentimiento\"]\n",
    "\n",
    "## Se dividen los atributos y los objetivos, en conjuntos de prueba y entrenamiento.\n",
    "(atributos_entrenamiento, atributos_prueba,\n",
    " objetivo_entrenamiento, objetivo_prueba) = train_test_split(\n",
    "       atributo, objetivo,\n",
    "       random_state=12345,\n",
    "       test_size=.2,\n",
    "       stratify=objetivo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ejercicio 5: ENTRENAMIENTO DEL MODELO\n",
    "\n",
    "El entrenamiento del modelo implica proporcionar un conjunto de atributos junto con sus objetivos correspondientes. De este modo, el modelo se ajusta y será capaz de generalizar a apartir de estos datos específicos, pudiendo predecir los objetivos de otros atributos. \n",
    "\n",
    "Al estar trabajando con texto codificado como vectores, generado mediante *Word2Vec*, se necesita realizar un modelo compatible, en concreto se eligió ***Random Forest***. Este algoritmo, en realidad, está formado por un conjunto de modelos individuales que se combinan para obtener un modelo más preciso; lo que supone una ventaja extra. \n",
    "\n",
    "Para ello, se importa *RandomForestClassifier* del módulo ensemble del paquete scikit-learn, anteriormente también utilizado para la codificación. Además, se carga el paquete numpy, utilizado para calcular la predicción del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A conituación, se genera el modelo usando *Random Forest* y los conjuntos de entrenamiento anteriormente extraidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo Random Forest es 71.8534961154273\n"
     ]
    }
   ],
   "source": [
    "## Se entrena el modelo Random Forest, con RandomForestClassifier y su función \"fit\".\n",
    "random_forest = RandomForestClassifier()\n",
    "random_forest.fit(list(atributos_entrenamiento[\"tweet_vector\"]), objetivo_entrenamiento)\n",
    "\n",
    "## Una vez generado el modelo se obtienen las predicciones del conjunto de prueba; usando la función \"predict\".\n",
    "predicciones_rf = rf_classifier.predict(list(atributos_prueba[\"tweet_vector\"]))\n",
    "\n",
    "## Se calcula la predicción del modelo. Para ello, se cuenta el número de veces que las predicciones generadas son equivalentes \n",
    "## a los objetivos reales y se normaliza, usando el paquete numpy.\n",
    "precision_rf = numpy.mean(predicciones_rf == objetivo_prueba)\n",
    "print(\"La precisión del modelo es\", precision_rf * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir del data set procesado y limpio, usando *Word2Vec* como codificador y *Random Forest*, se ha conseguido obtener un modelo con un **71.85% de predicción**. Para aumentar este valor, se podría entrenar el modelo con un conjunto de datos más grande. Sin embargo, se trata de una predicción alta y suficiente para dar por válido el modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "atributos_1=tweet_fifa_sent[\"tweet\"]\n",
    "vectorizer = CountVectorizer()\n",
    "atributos_1 = vectorizer.fit_transform(atributos_1)\n",
    "\n",
    "#Extrae el objetivo (ya es numérico)\n",
    "objetivo_1= tweet_fifa_sent[\"sentimiento\"]\n",
    "#Separa conjunto de entrenamiento y de prueba\n",
    "(atributos_entrenamiento_1, atributos_prueba_1,\n",
    " objetivo_entrenamiento_1, objetivo_prueba_1) = train_test_split(\n",
    "        atributos_1, objetivo_1,\n",
    "        random_state=12345,\n",
    "        test_size=.2,\n",
    "        stratify=objetivo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La precisión del modelo Naive Bayes desarrollado es 71.03218645948945\n"
     ]
    }
   ],
   "source": [
    "#### NAIVE BAYES\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Entrena el modelo de Naive Bayes usando la instancia MultinomialNB que es recomendada \n",
    "#para este tipo de tareas\n",
    "emotion_detector = MultinomialNB(alpha=1.0)  # alpha es el parámetro de suavizado\n",
    "emotion_detector.fit(atributos_entrenamiento_1, objetivo_entrenamiento_1)\n",
    "\n",
    "#Realiza las predicciones con el conjunto de prueba\n",
    "predicciones = emotion_detector.predict(atributos_prueba_1)\n",
    "#Calcular la precisión del modelo\n",
    "precision = emotion_detector.score(atributos_prueba_1, objetivo_prueba_1)\n",
    "print(\"La precisión del modelo Naive Bayes desarrollado es\", precision*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "name": "Solución_01_nueva.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
