{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af0f71ac",
   "metadata": {},
   "source": [
    "## Introducción al Trabajo de Detección de SPAM con Inteligencia Artificial\n",
    "\n",
    "Desde hace décadas, la presencia de la inteligencia artificial (IA) en nuestras vidas ha sido una constante evolución, marcando hitos significativos en la forma en que interactuamos con la tecnología. Sin embargo, es en los últimos años que hemos presenciado un impresionante avance, especialmente en el campo del **procesamiento del lenguaje natural (PLN)**, que ha llevado a las IA a alcanzar niveles de sofisticación notables. La detección de correo no deseado o SPAM es un ejemplo destacado de cómo la IA ha transformado la manera en que abordamos problemas cotidianos. En este contexto, nos encontramos con algoritmos que no solo reconocen patrones y características en el texto, sino que también comprenden el contexto y la semántica de las comunicaciones escritas.\n",
    "\n",
    "Un referente fundamental en este viaje hacia la excelencia en el PLN es el paquete **scikit-learn** (sklearn) en Python. Este conjunto de herramientas proporciona implementaciones eficientes y accesibles de una variedad de algoritmos de aprendizaje automático, incluyendo aquellos utilizados para la clasificación de texto en tareas como la detección de SPAM. Además, la biblioteca **nltk (Natural Language Toolkit)** juega un papel crucial en este proceso. Brindando herramientas y recursos para el procesamiento del lenguaje natural, nltk facilita la manipulación y comprensión de textos, permitiendo a los desarrolladores aprovechar técnicas avanzadas de PLN.\n",
    "\n",
    "En este script, exploraremos la aplicación de estas tecnologías, destacando cómo la convergencia de la inteligencia artificial, sklearn y nltk, ha revolucionado nuestra capacidad para identificar y filtrar eficientemente contenido no deseado en nuestros correos electrónicos. Al sumergirnos en la teoría y práctica detrás de estos avances, examinaremos cómo estos modelos, alimentados con grandes cantidades de datos etiquetados, pueden discernir entre mensajes legítimos y aquellos destinados a inundar nuestras bandejas de entrada con SPAM. Este viaje nos permitirá apreciar la extraordinaria capacidad de las IA modernas para procesar el texto natural, casi emulando la comprensión humana del lenguaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23904571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cargan todas las librerías que vamos a usar\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c44d1d03",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 29486",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Lectura de tablas de datos con panda\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcombined_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m data[\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m5\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1700\u001b[0m     (\n\u001b[0;32m   1701\u001b[0m         index,\n\u001b[0;32m   1702\u001b[0m         columns,\n\u001b[0;32m   1703\u001b[0m         col_dict,\n\u001b[1;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m         nrows\n\u001b[0;32m   1706\u001b[0m     )\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:875\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:850\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:861\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:2029\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 29486"
     ]
    }
   ],
   "source": [
    "#Lectura de tablas de datos con panda\n",
    "data = pd.read_csv('combined_data.csv', header=None, names=['label', 'text'])\n",
    "data[1:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afd91fcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Extrae el atributo, en este caso el texto de nuestros correos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m atributos\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Vectoriza el texto del atributo con el codificador CountVectorizer, distinto al que usamos con la tarea\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#de cars\u001b[39;00m\n\u001b[0;32m      5\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "#Extrae el atributo, en este caso el texto de nuestros correos\n",
    "atributos=data[\"text\"]\n",
    "#Vectoriza el texto del atributo con el codificador CountVectorizer, distinto al que usamos con la tarea\n",
    "#de cars\n",
    "vectorizer = CountVectorizer()\n",
    "#Convierte nuestros atributos en valores numéricos\n",
    "atributos = vectorizer.fit_transform(atributos)\n",
    "\n",
    "#Extrae el objetivo (ya es numérico)\n",
    "objetivo=data[\"label\"]\n",
    "\n",
    "#Separa conjunto de entrenamiento y de prueba\n",
    "(atributos_entrenamiento, atributos_prueba,\n",
    " objetivo_entrenamiento, objetivo_prueba) = train_test_split(\n",
    "        atributos, objetivo,\n",
    "        random_state=12345,\n",
    "        test_size=.2,\n",
    "        stratify=objetivo)\n",
    "\n",
    "\n",
    "#Entrena el modelo de Naive Bayes usando la instancia MultinomialNB que es recomendada \n",
    "#para este tipo de tareas\n",
    "spam_detector = MultinomialNB(alpha=1.0)  # alpha es el parámetro de suavizado\n",
    "spam_detector.fit(atributos_entrenamiento, objetivo_entrenamiento)\n",
    "\n",
    "#Realiza las predicciones con el conjunto de prueba\n",
    "predicciones = spam_detector.predict(atributos_prueba)\n",
    "#Calcular la precisión del modelo\n",
    "precision = spam_detector.score(atributos_prueba, objetivo_prueba)\n",
    "print(\"La precisión del modelo desarrollado es\", precision*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdcd030",
   "metadata": {},
   "source": [
    "Una vez que nuestro modelo ha sido entrenado con datos significativos y ha internalizado patrones distintivos asociados con correos SPAM y no SPAM, se convierte en una herramienta poderosa para la detección automatizada. Al aplicar este modelo a nuevos conjuntos de correos electrónicos, el proceso se vuelve tan simple como enviar el texto del mensaje a través del modelo previamente entrenado. Este acto de inferencia permite que el modelo evalúe rápidamente las características del texto en cuestión, clasificando el correo electrónico como potencialmente dañino o legítimo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2196537",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc7f01abe439>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0memail\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"¡Felicidades! Has ganado un premio increíble. Haz clic aquí para reclamar tu premio.\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#Conviertelo en valores numérico\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0memail_bien\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memail\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#ahora no se usa el el fit, ya que el codificador ya se creó al principio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#solo hay que volver a usarlo pero sin crear uno nuevo.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "#Hago la prueba con un email\n",
    "email=[\"¡Felicidades! Has ganado un premio increíble. Haz clic aquí para reclamar tu premio.\"]\n",
    "#Conviertelo en valores numérico\n",
    "email_bien = vectorizer.transform(email) \n",
    "#ahora no se usa el el fit, ya que el codificador ya se creó al principio\n",
    "#solo hay que volver a usarlo pero sin crear uno nuevo.\n",
    "\n",
    "#Predice con el modelo\n",
    "deteccion_email= spam_detector.predict(email_bien)\n",
    "\n",
    "print(f\"Texto del correo: {email[0]}\")\n",
    "print(f\"Predicción: {deteccion_email[0]} (Con un rango de 1 como spam y 0 no spam)\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
